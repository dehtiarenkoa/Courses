{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.615220521841592\n",
      "1.3862943611198906\n",
      "0.8085580320712731\n",
      "0.8615658321849085\n"
     ]
    }
   ],
   "source": [
    "#2.2.5\n",
    "import math\n",
    "y = [[0 for i in range(2)] for i in range(4)]\n",
    "y[0] = [0.99,0.01]\n",
    "y[1] = [0.5,0.5]\n",
    "y[2] = [0.99,0.45]\n",
    "y[3] = [0.65,0.65]\n",
    "for i in y:\n",
    "    print(-math.log(i[0])-math.log(i[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0237129365887834\n"
     ]
    }
   ],
   "source": [
    "#2.3.2\n",
    "import sympy.parsing.sympy_parser\n",
    "\n",
    "# from sympy import *\n",
    "# x, y, z, t = symbols('x y z t')\n",
    "x, y, w, b = symbols('x y w b')\n",
    "# k, m, n = symbols('k m n', integer=True)\n",
    "# f, g, h = symbols('f g h', cls=Function)\n",
    "\n",
    "# f = diff(cos(x), x)\n",
    "t =1/(1+exp(-w*x-b))\n",
    "sample_expr_str = f'diff(-y*log({t})-(1-y)*log(1-({t})), w)'\n",
    "sample_expr = sympy.parsing.sympy_parser.parse_expr(sample_expr_str)\n",
    "sample_value = sample_expr.evalf(subs=dict(x=0.5, y=1, w=4, b=1))\n",
    "print(sample_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right answer\n",
    "diff(-y*log(1/(1+exp(-w*x-b)))-(1-y)*log(1-(1/(1+exp(-w*x-b)))), w)\n",
    "diff(-y*log(1/(1+exp(-w*x-b)))-(1-y)*log(1-(1/(1+exp(-w*x-b)))), b).simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.5\n",
    "w - t*diff(-y*log(1/(1+exp(-w*x-b)))-(1-y)*log(1-(1/(1+exp(-w*x-b))))+c*(w**2+b**2), w).simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a318ef9c0923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m  \u001b[1;31m# ваше решение\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mpmi_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_pmi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a318ef9c0923>\u001b[0m in \u001b[0;36mread_array\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_pmi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a318ef9c0923>\u001b[0m in \u001b[0;36mparse_array\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a318ef9c0923>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "#2.3.7\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array([int(s.strip()) for s in s.strip().split(' ')])\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def calculate_pmi(a, b):\n",
    "    p_a = sum(a)/len(a)\n",
    "    p_b = sum(b)/len(b)\n",
    "    #p = len (set(a)&set(b))  #intersection\n",
    "    c = [1 if a[i]==1 and b[i]==1 else 0 for i in range(len(a)) ]\n",
    "    p_c = sum(c)/len(c)\n",
    "    res = np.log(p_c/p_a/p_b)\n",
    "    return res  # ваше решение\n",
    "\n",
    "a = read_array()\n",
    "b = read_array()\n",
    "pmi_value = calculate_pmi(a, b)\n",
    "\n",
    "print('{:.6f}'.format(pmi_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006452579827864143"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su = 0\n",
    "for i in range(1,11):\n",
    "    su+=i**(-2)\n",
    "#su\n",
    "r = 1/(su*100)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-da2b3dbcfe50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msu\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msu\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mro\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "su = 0\n",
    "r = 0\n",
    "ro = 0\n",
    "for j in range(1,12):\n",
    "    for i in range(1,j):\n",
    "        su+=i**(-2)\n",
    "    r = 1/(su*100)\n",
    "    ro +=r\n",
    "ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6439335666815615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.663679512615907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stepik.org/lesson/261086/step/8?discussion=1427903&unit=241830\n",
    "# считаем сначала Z. Можно так: sum([1/x**2 for x in range(1,1000)])\n",
    "# далее решаем уравнение \n",
    "# 1/(Z*k**s) < 0.001 (10/10000)\n",
    "# или Z*k**2 <1000\n",
    "# находим k (округляем в меньшую сторону) и вычитаем из 1000 k\n",
    "su = 0\n",
    "r = 0\n",
    "ro = 0\n",
    "for i in range(1,1000):\n",
    "    su+=i**(-2)\n",
    "print(su) #1.6439335666815615\n",
    "d = 1/(100*su)\n",
    "k = (1000/su)**0.5\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какого ;%:?* тут происходит?\n",
      "какого ; % : ? * тут происходит ?\n"
     ]
    }
   ],
   "source": [
    "#2.4.3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# модифицируйте это регулярное выражение\n",
    "#TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "TOKENIZE_RE = re.compile(r'[а-яё]+|-?\\d*[.,]?\\d+|[d*]|\\S', re.I)#[:punct:]|\n",
    "\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt)\n",
    "\n",
    "\n",
    "#for line in sys.stdin:\n",
    "s = input()\n",
    "print(' '.join(tokenize(s.strip().lower())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N = sys.stdin.readline()\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Контактный телефон: 123123. \tконтактный телефон : 123123 .\n",
    "Что-нибудь надо придумать. \tчто - нибудь надо придумать .\n",
    "Значение числа Е=2.7182. \tзначение числа е = 2.7182 .\n",
    "Демон123, как тебя зовут в реале? \tдемон 123 , как тебя зовут в реале ?\n",
    "-1-.15=-1.15 \t-1 -.15 = -1.15\n",
    "- 1 - .15 = -1.15 \t- 1 - .15 = -1.15\n",
    "Какого ;%:?* тут происходит? \tкакого ; % : ? * тут происходит ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать \n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4.5\n",
    "import collections, re\n",
    "\n",
    "s=[\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "size = 0\n",
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt.lower()) \n",
    "\n",
    "d = collections.defaultdict(int)\n",
    "#print (d)\n",
    "for line in s:\n",
    "    t = tokenize(line)\n",
    "    size+=1\n",
    "    unique = set(t)\n",
    "    for token in unique:\n",
    "        d[token]+=1\n",
    "ds = {k:(v/size) for k,v  in d.items() }\n",
    "ds = sorted(ds.items(), key = lambda pair:(pair[1],pair[0]))\n",
    "#print (ds)\n",
    "[print(k[0], end = \" \") for k in ds]\n",
    "print ()\n",
    "[print(k[1], end = \" \") for k in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('казнить', 1.6931471805599454),\n",
       " ('наказывать', 2.386294361119891),\n",
       " ('не', 2.386294361119891),\n",
       " ('нельзя', 1.2876820724517808),\n",
       " ('обязательно', 2.386294361119891),\n",
       " ('освободить', 1.6931471805599454),\n",
       " ('помиловать', 1.2876820724517808)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stepik.org/lesson/256724/step/5?discussion=1545582&unit=237035\n",
    "corpus = [\n",
    "    'Казнить нельзя, помиловать. Нельзя наказывать.',\n",
    "    'Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "    'Нельзя не помиловать.',\n",
    "    'Обязательно освободить.']\n",
    "\n",
    "## Получение df\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, use_idf=True)\n",
    "vectorizer.fit_transform(corpus)\n",
    "word_doc_freq = 1/np.exp(vectorizer.idf_ - 1) ## ВОТ ОСОБЕННОСТЬ ПЕРЕВОДА ИЗ IDF В DF\n",
    "#Словарь тут: \n",
    "vectorizer.get_feature_names()\n",
    "vectorizer.idf_\n",
    "b = zip(vectorizer.get_feature_names(),vectorizer.idf_)\n",
    "list(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('наказывать', 0.25), ('не', 0.25), ('обязательно', 0.25), ('казнить', 0.5), ('освободить', 0.5), ('нельзя', 0.75), ('помиловать', 0.75)]\n",
      "[(0, ('наказывать', 0.25)), (1, ('не', 0.25)), (2, ('обязательно', 0.25)), (3, ('казнить', 0.5)), (4, ('освободить', 0.5)), (5, ('нельзя', 0.75)), (6, ('помиловать', 0.75))]\n",
      "{'наказывать': 0, 'не': 1, 'обязательно': 2, 'казнить': 3, 'освободить': 4, 'нельзя': 5, 'помиловать': 6}\n",
      "[0.25 0.25 0.25 0.5  0.5  0.75 0.75]\n",
      "наказывать не обязательно казнить освободить нельзя помиловать \n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4.5\n",
    "import collections, re, numpy as np\n",
    "\n",
    "s=[\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "size = 0\n",
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt.lower()) \n",
    "\n",
    "d = collections.defaultdict(int)\n",
    "#print (d)\n",
    "for line in s:\n",
    "    t = tokenize(line)\n",
    "    size+=1\n",
    "    unique = set(t)\n",
    "    for token in unique:\n",
    "        d[token]+=1\n",
    "ds = {k:(v/size) for k,v  in d.items() }\n",
    "ds = sorted(ds.items(), key = lambda pair:(pair[1],pair[0]))\n",
    "print (ds)\n",
    "print(list(enumerate(ds)))\n",
    "w = {word: i for i, (word, _) in enumerate(ds)}\n",
    "print (w)\n",
    "w2 = np.array([cnt for _, cnt in ds], dtype='float32')\n",
    "print (w2)\n",
    "[print(k, end = \" \") for k in w]\n",
    "print ()\n",
    "[print(k, end = \" \") for k in w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать:0 не:1 обязательно:2 казнить:3 освободить:4 нельзя:5 помиловать:6 0 1 2 3 4 5 6 \n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4.5\n",
    "import collections, re, numpy as np\n",
    "\n",
    "s=[\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "size = 0\n",
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt.lower()) \n",
    "\n",
    "d = collections.defaultdict(int)\n",
    "#print (d)\n",
    "for line in s:\n",
    "    t = tokenize(line)\n",
    "    size+=1\n",
    "    unique = set(t)\n",
    "    for token in unique:\n",
    "        d[token]+=1\n",
    "ds = {k:(v/size) for k,v  in d.items() }\n",
    "ds = sorted(ds.items(), key = lambda pair:(pair[1],pair[0]))\n",
    "#print (ds)\n",
    "#print(list(enumerate(ds)))\n",
    "w = {word: i for i, (word, _) in enumerate(ds)}\n",
    "#print (w)\n",
    "w2 = np.array([cnt for _, cnt in ds], dtype='float32')\n",
    "#print (w2)\n",
    "[print(f'{k}:{v}', end = \" \") for k,v in w.items()]\n",
    "print ()\n",
    "[print(f'{k}', end = \" \") for k,v in w.items()]\n",
    "print ()\n",
    "[print(k, end = \" \") for k in w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'наказывать': 0,\n",
       "  'не': 1,\n",
       "  'обязательно': 2,\n",
       "  'казнить': 3,\n",
       "  'освободить': 4,\n",
       "  'помиловать': 5,\n",
       "  'нельзя': 6},\n",
       " array([0.25, 0.25, 0.25, 0.5 , 0.5 , 0.75, 0.75], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/dlnlputils/data/base.py#L34\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "texts = [\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=1):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token='<PAD>'):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=1, min_count=1, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                #reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "build_vocabulary(tokenize_corpus(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "import collections, re, numpy as np\n",
    "\n",
    "s=[\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "size = 0\n",
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt.lower()) \n",
    "\n",
    "d = collections.defaultdict(int)\n",
    "#print (d)\n",
    "for line in s:\n",
    "    t = tokenize(line)\n",
    "    size+=1\n",
    "    unique = set(t)\n",
    "    for token in unique:\n",
    "        d[token]+=1\n",
    "ds = {k:(v/size) for k,v  in d.items() }\n",
    "d1 = [(k,(v/size)) for k,v  in d.items() ]\n",
    "##ds = sorted(ds.items(), key = lambda pair:(pair[1],pair[0]))\n",
    "#print (ds)\n",
    "#print(list(enumerate(ds)))\n",
    "##w = {word: i for i, (word, _) in enumerate(ds)}\n",
    "#print (w)\n",
    "##w2 = np.array([cnt for _, cnt in ds], dtype='float32')\n",
    "#print (w2)\n",
    "# [print(f'{k}', end = \" \") for k,v in w.items()]\n",
    "# print ()\n",
    "# [print(k, end = \" \") for k in w2]\n",
    "#https://stepik.org/lesson/256724/step/5?discussion=1265959&unit=237035\n",
    "answer = sorted(d1, key=lambda x:(x[1],x[0]))\n",
    "#print(answer)\n",
    "answer_1 = [] \n",
    "answer_2 = []\n",
    "\n",
    "for k, v in list(answer):\n",
    "    answer_1.append(k)\n",
    "    answer_2.append(str(v))\n",
    "    \n",
    "print(\" \".join(answer_1))\n",
    "print(\" \".join(answer_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'], ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'], ['нельзя', 'не', 'помиловать'], ['обязательно', 'освободить']]\n",
      "d\n",
      "defaultdict(<class 'int'>, {'нельзя': 3, 'наказывать': 1, 'помиловать': 3, 'казнить': 2, 'освободить': 2, 'не': 1, 'обязательно': 1})\n",
      "dd\n",
      "defaultdict(<class 'list'>, {'нельзя': [2, 2, 1], 'наказывать': [1], 'помиловать': [1, 1, 1], 'казнить': [1, 1], 'освободить': [1, 1], 'не': [1], 'обязательно': [1]})\n",
      "tf\n",
      "[('наказывать', 0.25), ('не', 0.25), ('обязательно', 0.25), ('казнить', 0.5), ('освободить', 0.5), ('нельзя', 0.75), ('помиловать', 0.75)]\n",
      "[0.22314355 0.22314355 0.22314355 0.40546511 0.40546511 0.55961579\n",
      " 0.55961579]\n",
      "w\n",
      "{'наказывать': 0, 'не': 1, 'обязательно': 2, 'казнить': 3, 'освободить': 4, 'нельзя': 5, 'помиловать': 6}\n",
      "feature_matrix\n",
      "[[1. 0. 0. 1. 0. 2. 1.]\n",
      " [0. 0. 0. 1. 1. 2. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 0. 0.]]\n",
      "result7\n",
      "  (0, 3)\t1.0\n",
      "  (0, 5)\t2.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 0)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 5)\t2.0\n",
      "  (1, 6)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (2, 5)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "8\n",
      "[[5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]]\n",
      "fm\n",
      "[5. 5. 3. 2.]\n",
      "9\n",
      "[[0.2        0.         0.         0.2        0.         0.4\n",
      "  0.2       ]\n",
      " [0.         0.         0.         0.2        0.2        0.4\n",
      "  0.2       ]\n",
      " [0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.33333333]\n",
      " [0.         0.         0.5        0.         0.5        0.\n",
      "  0.        ]]\n",
      "[[0.18232156 0.         0.         0.18232156 0.         0.33647224\n",
      "  0.18232156]\n",
      " [0.         0.         0.         0.18232156 0.18232156 0.33647224\n",
      "  0.18232156]\n",
      " [0.         0.28768207 0.         0.         0.         0.28768207\n",
      "  0.28768207]\n",
      " [0.         0.         0.40546511 0.         0.40546511 0.\n",
      "  0.        ]]\n",
      "result\n",
      "  (0, 0)\t0.18232156\n",
      "  (0, 3)\t0.18232156\n",
      "  (0, 5)\t0.33647224\n",
      "  (0, 6)\t0.18232156\n",
      "  (1, 3)\t0.18232156\n",
      "  (1, 4)\t0.18232156\n",
      "  (1, 5)\t0.33647224\n",
      "  (1, 6)\t0.18232156\n",
      "  (2, 1)\t0.2876821\n",
      "  (2, 5)\t0.2876821\n",
      "  (2, 6)\t0.2876821\n",
      "  (3, 2)\t0.4054651\n",
      "  (3, 4)\t0.4054651\n",
      "[0.25 0.25 0.25 0.5  0.5  0.75 0.75]\n",
      "idf\n",
      "[1.3333333333333333, 4.0, 1.3333333333333333, 2.0, 2.0, 4.0, 4.0]\n",
      "result\n",
      "  (0, 0)\t0.24309541781743366\n",
      "  (0, 3)\t0.3646431267261505\n",
      "  (0, 5)\t1.3458889722824097\n",
      "  (0, 6)\t0.729286253452301\n",
      "  (1, 3)\t0.3646431267261505\n",
      "  (1, 4)\t0.3646431267261505\n",
      "  (1, 5)\t1.3458889722824097\n",
      "  (1, 6)\t0.729286253452301\n",
      "  (2, 1)\t1.1507283449172974\n",
      "  (2, 5)\t1.1507283449172974\n",
      "  (2, 6)\t1.1507283449172974\n",
      "  (3, 2)\t0.5406201283137003\n",
      "  (3, 4)\t0.8109301924705505\n",
      "feature_matrix\n",
      "[[0.24309541 0.         0.         0.36464311 0.         1.34588895\n",
      "  0.72928623]\n",
      " [0.         0.         0.         0.36464311 0.36464311 1.34588895\n",
      "  0.72928623]\n",
      " [0.         1.15072829 0.         0.         0.         1.15072829\n",
      "  1.15072829]\n",
      " [0.         0.         0.54062014 0.         0.81093022 0.\n",
      "  0.        ]]\n",
      "[0.06077385 0.28768207 0.13515504 0.18232156 0.29389333 0.96062655\n",
      " 0.65232519]\n",
      "[[0.06077385 0.28768209 0.13515503 0.18232156 0.29389333 0.96062657\n",
      "  0.65232521]]\n",
      "[[ 0.18232156 -0.28768207 -0.13515504  0.18232156 -0.29389333  0.3852624\n",
      "   0.07696104]\n",
      " [-0.06077385 -0.28768207 -0.13515504  0.18232156  0.07074978  0.3852624\n",
      "   0.07696104]\n",
      " [-0.06077385  0.86304622 -0.13515504 -0.18232156 -0.29389333  0.19010174\n",
      "   0.4984031 ]\n",
      " [-0.06077385 -0.28768207  0.40546511 -0.18232156  0.51703688 -0.96062655\n",
      "  -0.65232519]]\n",
      "[3.46944695e-18 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 5.55111512e-17]\n",
      "[[0.06077385 0.28768209 0.13515503 0.18232156 0.29389333 0.96062657\n",
      "  0.65232521]]\n",
      "[[ 0.18232156 -0.28768209 -0.13515503  0.18232156 -0.29389333  0.3852624\n",
      "   0.07696104]\n",
      " [-0.06077385 -0.28768209 -0.13515503  0.18232156  0.0707498   0.3852624\n",
      "   0.07696104]\n",
      " [-0.06077385  0.86304626 -0.13515503 -0.18232156 -0.29389333  0.19010177\n",
      "   0.49840313]\n",
      " [-0.06077385 -0.28768209  0.4054651  -0.18232156  0.51703686 -0.96062657\n",
      "  -0.65232521]]\n",
      "[0.1215477  0.57536414 0.27031007 0.2105268  0.38517496 0.64699208\n",
      " 0.47811428]\n",
      "[[ 1.5        -0.5        -0.5         0.8660254  -0.76301256  0.59546695\n",
      "   0.16096788]\n",
      " [-0.5        -0.5        -0.5         0.8660254   0.18368219  0.59546695\n",
      "   0.16096788]\n",
      " [-0.5         1.5        -0.5        -0.8660254  -0.76301256  0.29382391\n",
      "   1.04243508]\n",
      " [-0.5        -0.5         1.5        -0.8660254   1.34234292 -1.4847578\n",
      "  -1.36437084]]\n"
     ]
    }
   ],
   "source": [
    "#2.4.7\n",
    "import collections, re, math, numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "s=[\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
    "    \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
    "    \"Нельзя не помиловать.\",\n",
    "    \"Обязательно освободить.\",]\n",
    "size = 0\n",
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt.lower()) \n",
    "\n",
    "d = collections.defaultdict(int)\n",
    "dd = collections.defaultdict(list)\n",
    "\n",
    "te = []\n",
    "for line in s:\n",
    "    t = tokenize(line)\n",
    "    te.append(t)\n",
    "    size+=1\n",
    "    unique = set(t)\n",
    "    for token in unique:\n",
    "        d[token]+=1\n",
    "        dd[token].append(t.count(token))\n",
    "print(te)\n",
    "print (\"d\")\n",
    "print (d)\n",
    "print (\"dd\")\n",
    "print (dd)\n",
    "tf = {k:(v/size) for k,v  in d.items() }\n",
    "tf = sorted(tf.items(), key = lambda pair:(pair[1],pair[0]))\n",
    "print (\"tf\")\n",
    "print (tf)\n",
    "ltf = np.array([math.log1p(i[1]) for i in tf])\n",
    "print(ltf)\n",
    "res = []\n",
    "num_docs = size\n",
    "num_feats = len(d)\n",
    "feature_matrix = np.zeros((num_docs, num_feats))\n",
    "result = scipy.sparse.dok_matrix((num_docs, num_feats), dtype='float32')\n",
    "w = {word: i for i, (word, _) in enumerate(tf)}\n",
    "print(\"w\")\n",
    "print(w)\n",
    "print(\"feature_matrix\")\n",
    "for i in range(num_docs):    \n",
    "    for j  in te[i]:        \n",
    "        if j in w:\n",
    "#             print(\"j,w[j]\")\n",
    "#             print(j,w[j])\n",
    "            feature_matrix[i][w[j]] += 1\n",
    "            result[i,w[j]] += 1\n",
    "#         print(\"feature_matrix[i]\")\n",
    "#         print(feature_matrix[i])\n",
    "print(feature_matrix)\n",
    "print(\"result7\")\n",
    "print(result)\n",
    "print(\"8\")\n",
    "print(result.sum(1))\n",
    "\n",
    "#f4 = f2(feature_matrix.sum(0))\n",
    "#f6=f5(result.sum(1))\n",
    "#print(f6)\n",
    "#print(np.vectorize(math.log1p(i for i in result.sum(1))))\n",
    "\n",
    "#fm = feature_matrix*f5\n",
    "print(\"fm\")\n",
    "print(np.sum(feature_matrix, axis = 1))\n",
    "print(\"9\")\n",
    "#print([math.log1p(result.sum(1))])\n",
    "result = result.multiply(1 / result.sum(1))\n",
    "feature_matrix = feature_matrix/np.sum(feature_matrix, axis = 1).reshape(4,1)\n",
    "print(feature_matrix)\n",
    "def f_1(x):\n",
    "    return math.log1p(x)\n",
    "f2 = np.vectorize(f_1)\n",
    "feature_matrix = f2(feature_matrix)\n",
    "print(feature_matrix)\n",
    "#result = result.multiply(1 / f3)  # разделить каждую строку на её длину   TF\n",
    "result1 = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину   TF\n",
    "result = f2(result.toarray()) ### doesnt work:TypeError: must be real number, not coo_matrix\n",
    "#result = coo_matrix(result, shape=(4, 4))\n",
    "result = scipy.sparse.dok_matrix(result,(num_docs, num_feats), dtype='float32')\n",
    "print(\"result\")\n",
    "print(result)\n",
    "#word2freq = 1\n",
    "word2freq = np.array([g[1] for g in tf])\n",
    "print(word2freq)\n",
    "idf = [size/len(v) for v in dd.values()]\n",
    "print(\"idf\")\n",
    "print(idf)\n",
    "feature_matrix = feature_matrix*idf\n",
    "result = result.multiply(idf)\n",
    "#word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "#result = result.multiply(1 / word2freq)#np.vectorize(word2freq))  # разделить каждый столбец на вес слова\n",
    "print(\"result\")\n",
    "print(result)\n",
    "#result1 = result1.multiply(1 / word2freq)#np.vectorize(word2freq))  # разделить каждый столбец на вес слова\n",
    "print(\"feature_matrix\")\n",
    "print(feature_matrix)\n",
    "\n",
    "mean1 = feature_matrix.mean(0)\n",
    "print(mean1)\n",
    "mean = result.mean(0)\n",
    "print(mean)\n",
    "feature_matrix = feature_matrix - mean1\n",
    "print(feature_matrix)\n",
    "mean1 = feature_matrix.mean(0)\n",
    "print(mean1)\n",
    "mean = result.mean(0)\n",
    "print(mean)\n",
    "result = result - mean\n",
    "print(result)\n",
    "\n",
    "feats_std1 = feature_matrix.std(0, ddof=1)\n",
    "print(feats_std1)\n",
    "feature_matrix = feature_matrix/feats_std1\n",
    "print(feature_matrix)\n",
    "#feats_std = result.toarray().std(0, ddof=1) ###??? wtf: AttributeError: 'matrix' object has no attribute 'toarray'\n",
    "#print(feats_std)\n",
    "#[0.1215477  0.57536414 0.27031007 0.2105268  0.38517496 0.64699208 0.47811428]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  1.9 2.8 3.7 4.6] [1 1 2 3 4]\n",
      "[1.  1.9 2.8 3.7 4.6] [1 1 2 3 4]\n",
      "[1.  1.9 2.8 3.7 4.6] [1 1 2 3 4]\n",
      "[1.  1.9 2.8 3.7 4.6] [1 1 2 3 4]\n",
      "[1.  1.9 2.8 3.7 4.6] [1 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "#1.5  -0.5 -0.5 0.87 -0.76 0.6 0.16\n",
    "#0.5 -0.5 -0.5 0.87 0.18  0.595 0.1609\n",
    "\n",
    "def f_1(x):\n",
    "    return np.int(x)\n",
    "\n",
    "for t in range(5):\n",
    "    f2 = np.vectorize(f_1)\n",
    "\n",
    "    x = np.arange(1, 5, 0.9)\n",
    "    print(x, f2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/dlnlputils/data/bag_of_words.py\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()\n",
    "\n",
    "\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf = np.arange(2,6).reshape(2,2)\n",
    "print(gf)\n",
    "gf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function arange in module numpy:\n",
      "\n",
      "arange(...)\n",
      "    arange([start,] stop[, step,], dtype=None)\n",
      "    \n",
      "    Return evenly spaced values within a given interval.\n",
      "    \n",
      "    Values are generated within the half-open interval ``[start, stop)``\n",
      "    (in other words, the interval including `start` but excluding `stop`).\n",
      "    For integer arguments the function is equivalent to the Python built-in\n",
      "    `range` function, but returns an ndarray rather than a list.\n",
      "    \n",
      "    When using a non-integer step, such as 0.1, the results will often not\n",
      "    be consistent.  It is better to use `numpy.linspace` for these cases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : number, optional\n",
      "        Start of interval.  The interval includes this value.  The default\n",
      "        start value is 0.\n",
      "    stop : number\n",
      "        End of interval.  The interval does not include this value, except\n",
      "        in some cases where `step` is not an integer and floating point\n",
      "        round-off affects the length of `out`.\n",
      "    step : number, optional\n",
      "        Spacing between values.  For any output `out`, this is the distance\n",
      "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
      "        step size is 1.  If `step` is specified as a position argument,\n",
      "        `start` must also be given.\n",
      "    dtype : dtype\n",
      "        The type of the output array.  If `dtype` is not given, infer the data\n",
      "        type from the other input arguments.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    arange : ndarray\n",
      "        Array of evenly spaced values.\n",
      "    \n",
      "        For floating point arguments, the length of the result is\n",
      "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
      "        this rule may result in the last element of `out` being greater\n",
      "        than `stop`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    linspace : Evenly spaced numbers with careful handling of endpoints.\n",
      "    ogrid: Arrays of evenly spaced numbers in N-dimensions.\n",
      "    mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.arange(3)\n",
      "    array([0, 1, 2])\n",
      "    >>> np.arange(3.0)\n",
      "    array([ 0.,  1.,  2.])\n",
      "    >>> np.arange(3,7)\n",
      "    array([3, 4, 5, 6])\n",
      "    >>> np.arange(3,7,2)\n",
      "    array([3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    " help(np.arange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 1, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfunc(a, b):\n",
    "    \"Return a-b if a>b, otherwise return a+b\"\n",
    "    if a > b:\n",
    "         return a - b\n",
    "    else:\n",
    "         return a + b\n",
    "\n",
    "vfunc = np.vectorize(myfunc)\n",
    "vfunc([1, 2, 3, 4], 2)\n",
    "#array([3, 4, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done!\n",
    "the right answer :\n",
    "1.5   -0.5 -0.5  0.8660254 -0.76301256 0.59546695 0.16096788\n",
    "-0.5 -0.5 -0.5 0.8660254 0.18368219 0.59546695 0.16096788\n",
    "-0.5 1.5 -0.5 -0.8660254 -0.76301256 0.29382391 1.04243508\n",
    "-0.5 -0.5 1.5 -0.8660254 1.34234292 -1.4847578 -1.36437084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 2, 0]\n",
      "[1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#3.6.6\n",
    "import numpy as np\n",
    "x = np.array([[1, 0], \n",
    "              [1, 1], \n",
    "              [0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0]])\n",
    "Kernel = np.array([[1,1, 0], \n",
    "                   [0,1,1]])\n",
    "  \n",
    "y = [0,0,0]\n",
    "pos = 0\n",
    "for k in range(3):  \n",
    "    print(y)\n",
    "    for ic in range (2):\n",
    "        y[k]+=x[pos+k,ic]*Kernel[ic,k]\n",
    "print(y)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 1]\n",
      " [1 0]]\n",
      "x[pos:pos+3,:] [[1 0]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "[0 2 0]\n",
      "x[pos:pos+3,:] [[1 1]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "[[0 1]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[1 0 0]\n",
      "x[pos:pos+3,:] [[0 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[0 1 1]\n",
      "[1 3 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 0], \n",
    "              [1, 1], \n",
    "              [0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0]])\n",
    "# Kernel = np.array([ [1,1,0], \n",
    "#                     [0,1,1]])\n",
    "Kernel = np.array([ [0,1,1], \n",
    "                     [1,1,0]])\n",
    "kt = Kernel.T\n",
    "print(kt)\n",
    "y =w = [0,0,0]\n",
    "#pos = 1\n",
    "for pos in range(3):# 3 <- Kernel.shape[1]\n",
    "    #for k in range(3):  \n",
    "\n",
    "    c = np.multiply(x[pos:pos+3,:],kt)\n",
    "    print(\"x[pos:pos+3,:]\",x[pos:pos+3,:])\n",
    "    print(c)\n",
    "    y = np.sum(c, axis = 1)\n",
    "    print(y)\n",
    "    w +=y \n",
    "#         for ic in range (2):\n",
    "#             y[k]+=x[pos+k,ic]*Kernel[ic,k]\n",
    "#print(y)    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kernel.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 2, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 0], \n",
    "              [1, 1], \n",
    "              [0, 0],] \n",
    "              )\n",
    "Kernel = np.array([ [1,1,0], \n",
    "                    [0,1,1]])\n",
    "x.dot(Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#done!\n",
    "import numpy as np\n",
    "x = np.array([[1, 0], \n",
    "              [1, 1], \n",
    "              [0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0]])\n",
    "Kernel = np.array([[1,1, 0], \n",
    "                   [0,1,1]])\n",
    "  \n",
    "y = [0,0,0]\n",
    "pos = 0\n",
    "for pos in range(3):\n",
    "    for k in range(3):  \n",
    "        print(y[pos])\n",
    "        for ic in range (2):\n",
    "            y[pos]+=x[pos+k,ic]*Kernel[ic,k]\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[3, 1], [2, 2], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "#3.6.7 done!\n",
    "import numpy as np\n",
    "x = np.array([[1, 0], \n",
    "              [1, 1], \n",
    "              [0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0]])\n",
    "Kernel = np.array([[[1,1, 0], \n",
    "                   [0,1,1]],\n",
    "                  [[1,0, 0], \n",
    "                   [0,0,1]]])\n",
    "  \n",
    "y = [[0,0],\n",
    "     [0,0],\n",
    "     [0,0]]\n",
    "pos = 0\n",
    "for pos in range(3):\n",
    "    for j in range(2):  \n",
    "        print(y[pos][j])\n",
    "        for k in range(3):\n",
    "            for ic in range (2):\n",
    "                y[pos][j]+=x[pos+k,ic]*Kernel[j,ic,k]\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 1 2 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#4.5.3\n",
    "import numpy as np\n",
    "W = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "W.T\n",
    "a = W.dot(W.T)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ac1b6e60c02d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Docs\\python\\WPy64-3800\\python-3.8.0.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   3021\u001b[0m   \"\"\"\n\u001b[0;32m   3022\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3023\u001b[1;33m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[0;32m   3024\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "#https://overcoder.net/q/39323/%D0%BA%D0%B0%D0%BA-%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D1%8C-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8E-softmax-%D0%B2-python\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "batch = np.asarray(a)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y = tf.nn.softmax(x)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(y, feed_dict={x: batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 1 2 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "W.T\n",
    "a = W.dot(W.T)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36552929, 0.13447071, 0.36552929, 0.13447071],\n",
       "       [0.13447071, 0.36552929, 0.36552929, 0.13447071],\n",
       "       [0.19661193, 0.19661193, 0.53444665, 0.07232949],\n",
       "       [0.25      , 0.25      , 0.25      , 0.25      ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at = a.T\n",
    "def softmax(x):\n",
    "   return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "\n",
    "np.transpose(softmax(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch===1.5.0 in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torch===1.5.0) (0.18.0)\n",
      "Requirement already satisfied: numpy in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torch===1.5.0) (1.17.3+mkl)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-win_amd64.whl\n",
    "#!pip install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torch===1.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3655, 0.1345, 0.3655, 0.1345],\n",
      "        [0.1345, 0.3655, 0.3655, 0.1345],\n",
      "        [0.1966, 0.1966, 0.5344, 0.0723],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sm_rows = torch.nn.Softmax(dim = 1)\n",
    "c= sm_rows(torch.from_numpy(a).type(torch.FloatTensor))\n",
    "print(c)\n",
    "#x = x.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.from_numpy(W).type(torch.FloatTensor)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.5000],\n",
       "        [0.5000, 0.7311],\n",
       "        [0.7311, 0.7311],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.5.5 done!\n",
    "r = torch.mm(c,s)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# os.add_dll_directory(\"E:\\Docs\\python\\WPy64-3800\\python-3.8.0.amd64\\lib\\site-packages\\torch\\lib\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch===1.5.0 in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (1.5.0)\n",
      "Collecting torchvision===0.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.6.0-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: future in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torch===1.5.0) (0.18.0)\n",
      "Requirement already satisfied: numpy in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torch===1.5.0) (1.17.3+mkl)\n",
      "Requirement already satisfied: pillow>=4.1.1 in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torchvision===0.6.0) (6.2.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyder 3.3.6 requires pyqt5, which is not installed.\n",
      "pyqtwebengine 5.13.1 requires pyqt5, which is not installed.\n",
      "uvicorn 0.9.0 has requirement h11==0.8.*, but you have h11 0.9.0.\n",
      "spyder 3.3.6 has requirement pyqtwebengine<5.13; python_version >= \"3\", but you have pyqtwebengine 5.13.1.\n"
     ]
    }
   ],
   "source": [
    "!pip check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: torch\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.4.0+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp38-cp38-win_amd64.whl (77.4 MB)\n",
      "Collecting torchvision==0.5.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.5.0%2Bcpu-cp38-cp38-win_amd64.whl (485 kB)\n",
      "Requirement already satisfied: six in c:\\users\\aero\\appdata\\roaming\\python\\python38\\site-packages (from torchvision==0.5.0+cpu) (1.13.0)\n",
      "Requirement already satisfied: numpy in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torchvision==0.5.0+cpu) (1.17.3+mkl)\n",
      "Requirement already satisfied: pillow>=4.1.1 in e:\\docs\\python\\wpy64-3800\\python-3.8.0.amd64\\lib\\site-packages (from torchvision==0.5.0+cpu) (6.2.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.4.0+cpu torchvision-0.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 1 2 0]\n",
      " [0 0 0 0]]\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "Keys\n",
      " [[1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "Queries\n",
      " [[0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]]\n",
      "Values\n",
      " [[1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "Logits\n",
      " [[0 0 0 0]\n",
      " [1 0 1 0]\n",
      " [1 0 1 0]\n",
      " [0 0 0 0]]\n",
      "Logits\n",
      " [[0 0 0 0]\n",
      " [1 0 1 0]\n",
      " [1 0 1 0]\n",
      " [0 0 0 0]]\n",
      "Result\n",
      " [[0.5        0.5       ]\n",
      " [0.73105858 0.5       ]\n",
      " [0.73105858 0.5       ]\n",
      " [0.5        0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "#2.5.7 done\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "Input = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "Input.T\n",
    "a = Input.dot(Input.T)\n",
    "print(a)\n",
    "\n",
    "# def softmax(x):\n",
    "#    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "\n",
    "ProjK = np.array([[1, 0], [0, 1]])\n",
    "ProjQ = np.array([[0, 0], [1, 0]])\n",
    "ProjV = np.array([[1, 0], [0, 1]])\n",
    "BiasK = np.array([0,0])\n",
    "BiasQ = np.array([0,0])\n",
    "BiasV = np.array([0,0])\n",
    "#Keys=Input⋅ProjK+BiasK\n",
    "#Queries=Input⋅ProjQ+BiasQQueries = Input \\cdot Proj_Q + Bias_QQueries=Input⋅ProjQ​+BiasQ​\n",
    "#Values=Input⋅ProjV+BiasVValues = Input \\cdot Proj_V + Bias_VValues=Input⋅ProjV​+BiasV​\n",
    "#Logits=Queries⋅Keys.T\n",
    "#AttScores=softmax(Logits,rows)\n",
    "#Result=AttScores⋅Values\n",
    "Keys = Input.dot(ProjK)+BiasK\n",
    "print(Input.dot(ProjK))\n",
    "print(\"Keys\\n\",Keys)\n",
    "Queries = Input.dot(ProjQ)+BiasQ\n",
    "print(\"Queries\\n\",Queries)\n",
    "Values = Input.dot(ProjV)+BiasV\n",
    "print(\"Values\\n\",Values)\n",
    "Logits=Queries.dot(Keys.T)\n",
    "print(\"Logits\\n\",Logits)\n",
    "AttScores=softmax(Logits, axis = 1)\n",
    "print(\"Logits\\n\",Logits)\n",
    "Result=AttScores.dot(Values)\n",
    "print(\"Result\\n\",Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36552929, 0.13447071, 0.19661193, 0.25      ],\n",
       "       [0.13447071, 0.36552929, 0.19661193, 0.25      ],\n",
       "       [0.36552929, 0.36552929, 0.53444665, 0.25      ],\n",
       "       [0.13447071, 0.13447071, 0.07232949, 0.25      ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "# array([[0.36552929, 0.13447071, 0.19661193, 0.25      ],\n",
    "#        [0.13447071, 0.36552929, 0.19661193, 0.25      ],\n",
    "#        [0.36552929, 0.36552929, 0.53444665, 0.25      ],\n",
    "#        [0.13447071, 0.13447071, 0.07232949, 0.25      ]])\n",
    "\n",
    "# def softmax(x):\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "# array([[0.08313107, 0.03058221, 0.08313107, 0.03058221],\n",
    "#        [0.03058221, 0.08313107, 0.08313107, 0.03058221],\n",
    "#        [0.08313107, 0.08313107, 0.22597368, 0.03058221],\n",
    "#        [0.03058221, 0.03058221, 0.03058221, 0.03058221]])\n",
    "\n",
    "\n",
    "\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [1, 1, 2, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.43656366,  7.43656366, 13.82561976,  4.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08313107 0.03058221 0.08313107 0.03058221]\n",
      " [0.03058221 0.08313107 0.08313107 0.03058221]\n",
      " [0.08313107 0.08313107 0.22597368 0.03058221]\n",
      " [0.03058221 0.03058221 0.03058221 0.03058221]]\n",
      "[[0.36552929 0.13447071 0.19661193 0.25      ]\n",
      " [0.13447071 0.36552929 0.19661193 0.25      ]\n",
      " [0.36552929 0.36552929 0.53444665 0.25      ]\n",
      " [0.13447071 0.13447071 0.07232949 0.25      ]]\n",
      "[[0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " [0.19661193 0.19661193 0.53444665 0.07232949]\n",
      " [0.25       0.25       0.25       0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "m = softmax(a)\n",
    "print(m,end = \"\\n\")\n",
    "m0 = softmax(a, axis=0)\n",
    "print(m0,end = \"\\n\")\n",
    "m1 = softmax(a, axis=1)\n",
    "print(m1,end = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 1 2 0]\n",
      " [0 0 0 0]]\n",
      "ProjK[j] [[1 0]\n",
      " [0 0]] (2, 2) (2, 2, 2)\n",
      "[[1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]]\n",
      "Keys\n",
      " [[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "Queries\n",
      " [[0. 1.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [0. 0.]]\n",
      "Values\n",
      " [1. 0. 1. 0.]\n",
      "Logits\n",
      " [[0. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "AttScores\n",
      " [[0. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Result\n",
      " [0.5        0.73105858 0.73105858 0.5       ]\n",
      "ProjK[j] [[0 0]\n",
      " [1 0]] (2, 2) (2, 2, 2)\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]]\n",
      "Keys\n",
      " [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "Queries\n",
      " [[1. 0.]\n",
      " [1. 1.]\n",
      " [2. 1.]\n",
      " [0. 0.]]\n",
      "Values\n",
      " [0. 1. 1. 0.]\n",
      "Logits\n",
      " [[0. 1. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 2. 2. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "AttScores\n",
      " [[0. 1. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 2. 2. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Result\n",
      " [0.73105858 0.73105858 0.88079708 0.5       ]\n",
      "[[0.5        0.73105858 0.73105858 0.5       ]\n",
      " [0.73105858 0.73105858 0.88079708 0.5       ]]\n",
      "[[0.5        0.73105858]\n",
      " [0.73105858 0.73105858]\n",
      " [0.73105858 0.88079708]\n",
      " [0.5        0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "#4.5.9 done!\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "Input = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "Input.T\n",
    "a = Input.dot(Input.T)\n",
    "print(a)\n",
    "\n",
    "# def softmax(x):\n",
    "#    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "\n",
    "ProjK = np.array([[[1, 0], [0, 0]],\n",
    "                  [[0, 0], [1, 0]]])\n",
    "ProjQ = np.array([[[0, 1], [1, 1]],\n",
    "                  [[1, 0], [1, 1]]])\n",
    "ProjV = np.array([[1, 0],\n",
    "                  [0, 1]])\n",
    "BiasK = np.array([0,0])\n",
    "BiasQ = np.array([0,0])\n",
    "BiasV = np.array([0])\n",
    "#Keys=Input⋅ProjK+BiasK\n",
    "#Queries=Input⋅ProjQ+BiasQQueries = Input \\cdot Proj_Q + Bias_QQueries=Input⋅ProjQ​+BiasQ​\n",
    "#Values=Input⋅ProjV+BiasVValues = Input \\cdot Proj_V + Bias_VValues=Input⋅ProjV​+BiasV​\n",
    "#Logits=Queries⋅Keys.T\n",
    "#AttScores=softmax(Logits,rows)\n",
    "#Result=AttScores⋅Values\n",
    "Keys = np.zeros((2,4,2))\n",
    "Queries = np.zeros((2,4,2))\n",
    "Values = np.zeros((2,4))\n",
    "AttScores = np.zeros((2,4,4))\n",
    "Logits = np.zeros((2,4,4))\n",
    "Result = np.zeros((2,4))\n",
    "for j in range(2):\n",
    "    print(\"ProjK[j]\",ProjK[j],np.shape(ProjK[j]),np.shape(ProjK))\n",
    "    Keys[j] = Input.dot(ProjK[j])+BiasK\n",
    "    print(Input.dot(ProjK[j]))\n",
    "    print(\"Keys\\n\",Keys[j])\n",
    "    Queries[j] = Input.dot(ProjQ[j])+BiasQ\n",
    "    print(\"Queries\\n\",Queries[j])\n",
    "    Values[j] = Input.dot(ProjV[j])+BiasV\n",
    "    print(\"Values\\n\",Values[j])\n",
    "    Logits[j] =  Queries[j].dot(Keys[j].T)\n",
    "    print(\"Logits\\n\",Logits[j])\n",
    "    AttScores[j]=softmax(Logits[j], axis = 1)\n",
    "    print(\"AttScores\\n\",Logits[j])\n",
    "    Result[j]=AttScores[j].dot(Values[j])\n",
    "    print(\"Result\\n\",Result[j])\n",
    "d = np.stack((Result[0],Result[1]))\n",
    "print(d)\n",
    "print(d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
